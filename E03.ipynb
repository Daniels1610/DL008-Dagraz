{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03 Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "IMG_DIMS = (7,7)\n",
    "FILTER_DIMS = (3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.01** Calcular dimensiones espaciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensiones Espaciales:** si asumimos una entrada de $(7 \\times 7)$ y un filtro (campo receptivo) de $(3 \\times 3)$, la salida de la convolución será un arreglo de ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $(5 \\times 5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.02:** Calcular dimensiones del mapa de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizar una función que reciba como parámetros de entrada las dimensiones de la Imágen y las dimensiones del Filtro. La función deberá regresar como resultado (Salida) las dimensiones del mapa de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5]\n",
      "[5, 7]\n"
     ]
    }
   ],
   "source": [
    "def get_feature_map(img:tuple, f:tuple) -> tuple:\n",
    "    feature_dims = list((0,0))\n",
    "    for i in range(img[0]):\n",
    "        i += f[0] - 1\n",
    "        if (i < img[0]) : feature_dims[0] += 1\n",
    "    for j in range(img[1]):\n",
    "        j += f[1] - 1\n",
    "        if (j < img[1]): feature_dims[1] += 1\n",
    "    return feature_dims\n",
    "            \n",
    "print(get_feature_map(IMG_DIMS, FILTER_DIMS))\n",
    "print(get_feature_map((7,9), (3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.03:** Agregar $\\text{stride}$ a la función de la `E3.01`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensiones espaciales** : si asumimos la misma entrada y\n",
    "filtro del ejercicio 3.01, pero ahora incluimos un tamaño\n",
    "de paso entre un producto punto a otro de 2, la salida de\n",
    "la convolución será un arreglo de ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $(3 \\times 3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.04** Calcular Mapa de Características con $\\text{stride}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejorar la implementación del `E3.02`. Realizar una  \n",
    "función que reciba como parámetros de entrada las   \n",
    "dimensiones de la **Imagen**, las dimensiones del **Filtro**  \n",
    "y el tamaño del paso (stride). La función deberá  \n",
    "regresar como resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets Feature Maps Dimensions\n",
    "def get_fm(img:tuple, f:tuple, stride:np.int8) -> tuple:\n",
    "    feature_dims = np.empty(2)\n",
    "    feature_dims[0] = ((img[0] - f[0]) / stride) + 1\n",
    "    feature_dims[1] = ((img[1] - f[1]) / stride) + 1\n",
    "    return tuple(feature_dims)\n",
    "            \n",
    "print(get_fm(IMG_DIMS, FILTER_DIMS, 1))\n",
    "print(get_fm(IMG_DIMS, FILTER_DIMS, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultados:**  \n",
    "$7,7,3,3,2 | 3,3$  \n",
    "$7,7,3,3,1 | 5,5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.05:** Calcular convolución de imágen y filtro\n",
    "• **Dimensiones espaciales** : si asumimos la misma entrada y\n",
    "filtro, pero incluimos un tamaño de paso entre un\n",
    "producto punto a otro de 3, la salida de la convolución\n",
    "será un arreglo de ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.06:** Mejorar la implementación del Ejercicio 3.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar una función que reciba como parámetros de entrada las  \n",
    "dimensiones de la **Imagen**, las dimensiones del **Filtro** y el   \n",
    "tamaño del paso **(stride)**. La función deberá regresar como resultado  \n",
    "(Salida) las dimensiones del mapa de características y deberá **validar**   \n",
    "que el filtro se pueda ajustar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(3, 3)\n",
      "Operation is not valid!\n"
     ]
    }
   ],
   "source": [
    "def get_fm(img:tuple, f:tuple, stride:np.int8) -> tuple:\n",
    "    feature_dims = np.empty(2, dtype=np.int8)\n",
    "    if (((img[0] - f[0]) % stride != 0) or ((img[1] - f[1]) % stride != 0)):\n",
    "        return \"Operation is not valid!\"\n",
    "    else:\n",
    "        feature_dims[0] = (img[0] - f[0]) / stride + 1\n",
    "        feature_dims[1] = (img[1] - f[1]) / stride + 1\n",
    "    return tuple(feature_dims)\n",
    "\n",
    "print(get_fm(IMG_DIMS, FILTER_DIMS, 1))\n",
    "print(get_fm(IMG_DIMS, FILTER_DIMS, 2))\n",
    "print(get_fm(IMG_DIMS, FILTER_DIMS, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.07:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimensiones espaciales : si asumimos una entrada de 7x7, un filtro de 5x5, un\n",
    "tamaño de paso entre un producto punto a otro de 1 y agregamos un relleno de\n",
    "ceros ( zero padding ) de un pixel, la salida de la convolución será un arreglo de ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejorar la implementación del ejercicio 3.06. Realizar\n",
    "una función que reciba como parámetros de entrada las\n",
    "dimensiones de la Imagen , las dimensiones del Filtro , el\n",
    "tamaño del paso ( stride ) y la indicación si se incluye un relleno de ceros. La función deberá regresar como\n",
    "resultado ( Salida ) las dimensiones del mapa de\n",
    "características y deberá validar que el filtro se pueda\n",
    "ajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.09:** Recortar imágen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga la imágen de `china.jpg` disponible en `sklearn.datasets`  \n",
    "y realizarle un preprocesamiento, el cual consiste en recortar la  \n",
    "imagen y convertirla a escala de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = load_sample_image(\"china.jpg\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = china[150:220, 130:250]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = image.shape\n",
    "image_grayscale = image.mean(axis=2).astype(np.float32)\n",
    "image_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image_grayscale.reshape(1, height, width, 1)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(); ax.axis(\"off\")\n",
    "ax.imshow(image_grayscale, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un par de filtros, uno para líneas verticales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical = np.zeros((7,6))\n",
    "vertical = np.insert(vertical, vertical.shape[0]//2, np.ones(vertical.shape[0]), axis=1)\n",
    "vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal = np.zeros((6,7))\n",
    "horizontal = np.insert(horizontal, horizontal.shape[1]//2, np.ones(vertical.shape[0]), axis=0)\n",
    "horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10)) \n",
    "rows=1; cols=2\n",
    "\n",
    "fig.add_subplot(rows,cols, 1); \n",
    "plt.imshow(vertical, cmap='gray')\n",
    "\n",
    "fig.add_subplot(rows,cols, 2); \n",
    "plt.imshow(horizontal, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.stack((horizontal, vertical), axis=2)\n",
    "filters = np.reshape(filters, newshape=(horizontal.shape[0],vertical.shape[0],1,2))\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.10:** Implementar capa convolucional aplicada `china.jpg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar una capa convolucional empleando `tf.nn.conv2d`  \n",
    "y los filtros (`filters`) creados para obtener los mapas de   \n",
    "características de la imágen (`china.jpg`) pre-procesada (`images`).  \n",
    "Utilizar un paso de uno (`strides=1`) y un relleno de ceros (`padding=\"SAME\"`).\n",
    "\n",
    "Desplegar los mapas de características (salidas obtenidas  \n",
    "de la capa convolucional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(images, dtype=tf.float32)\n",
    "kernel = tf.constant(filters, dtype=tf.float32)\n",
    "a = tf.nn.conv2d(x, kernel, strides=1, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ver = np.reshape(a[:,:,:,1], newshape=(70,120))\n",
    "img_hoz = np.reshape(a[:,:,:,0], newshape=(70,120))\n",
    "img_ver.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 10)) \n",
    "\n",
    "axs[0].imshow(image_grayscale, cmap='gray')\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "axs[1].imshow(img_ver, cmap='gray')\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "axs[2].imshow(img_hoz, cmap='gray')\n",
    "axs[2].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.11:** Implementar convolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{i,j,k}= b_{k} + \\sum_{u=0}^{f_{h}-1} \\sum_{v=0}^{f_{w}-1} \\sum_{k'=0}^{f_{n'}-1} x_{i',j',k'} \\cdot w_{u,v,k',k}\\;\\;\\;with \\begin{cases} &  i'=  i \\times s_{h} + u\\\\   &  j' = j \\times s_{w} + v  \\end{cases}$    \n",
    "\n",
    "- Implementar una función para la expresión del cálculo de la salida de una neurona ($z_{i,j,k}$). Considere un  \n",
    "mapa de caracerísticas (imagen de entrada) con dimensiones $H \\times W \\times K$, con un campo receptivo (filtro)  \n",
    "con dimensiones $f_{h} \\times f_{w} \\times f_{n'}$. Considere el $stride$ como parámetro de entrada y deje fijo el sesgo en 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 6, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red = np.array([[0,0,0,0,0,0],\n",
    "                [0,156,155,156,158,158],\n",
    "                [0,153,154,157,159,159],\n",
    "                [0,146,151,155,158,159]])\n",
    "\n",
    "green = np.array([[0,0,0,0,0,0],\n",
    "                  [0,167,166,167,169,169],\n",
    "                  [0,164,165,168,170,170],\n",
    "                  [0,160,162,166,169,170]])\n",
    "\n",
    "blue = np.array([[0,0,0,0,0,0],\n",
    "                 [0,163,162,163,165,165],\n",
    "                 [0,160,161,164,166,166],\n",
    "                 [0,156,158,162,165,166]])\n",
    "\n",
    "channels = np.stack((red, green, blue), axis=2)\n",
    "channels = channels.reshape(1, red.shape[0], red.shape[1], 3)\n",
    "channels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_red = np.array([[-1,-1,1],[0,1,-1],[0,1,1]])\n",
    "filter_green = np.array([[1,0,0],[1,-1,-1],[1,0,-1]])\n",
    "filter_blue = np.array([[0,1,1],[0,1,0],[1,-1,1]])\n",
    "\n",
    "filters = np.stack((filter_red, filter_green, filter_blue), axis=2)\n",
    "filters = np.reshape(filters, newshape=(filter_red.shape[0],filter_red.shape[1],1,3))\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-25, 466, 466, 475],\n",
       "       [292, 787, 798, 812]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def z(channels:np.ndarray, filters:np.ndarray, stride:np.int8) -> np.ndarray:\n",
    "    # Get feature maps dimensions\n",
    "    fm_dims = get_fm((channels.shape[1],channels.shape[2]), (filters.shape[0],filters.shape[1]), stride)\n",
    "    \n",
    "    # Declare output array\n",
    "    z = np.zeros((fm_dims[0], fm_dims[1]), dtype=np.int64)\n",
    "    \n",
    "    # Filter Height (Rows) and Filter Width (Columns)\n",
    "    fh, fw = filters.shape[0:2]\n",
    "\n",
    "    # Compute dot product between filter and image's channels on \n",
    "    # each position the filters adjust\n",
    "    for i in range(fm_dims[0]):\n",
    "        for j in range(fm_dims[1]):\n",
    "            for k in range(channels.shape[3]):\n",
    "                z[i,j] += np.dot(\n",
    "                    channels[..., k][0,i:fh+i,j:fw+j].flatten(), \n",
    "                    filters[...,k].flatten())\n",
    "    \n",
    "    return z + 1\n",
    "\n",
    "z(channels, filters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.12:** Aplicar filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar las imágenes `china.jpg` y `flower.jpg` disponibles `sklearn.datasets`\n",
    "\n",
    "- Aplicar los filtros (líneas vertical y horizontal) a las imágenes\n",
    "utilizando una capa convolucional construida con la función `conv2d()`\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer\n",
    "x = tf.constant(images, dtype=tf.float32)\n",
    "kernel = tf.constant(filters, dtype=tf.float32)\n",
    "outputs = tf.nn.conv2d(x, kernel, strides=1, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E3.13**: Implementar Capa de Agrupación Máxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementar uan capa de agrupación máxima (max pooling layer) en Tensorflow es bastante sencillo.\n",
    "\n",
    "- El siguiente código crea una capa de agrupación máxima utilizando un kernel $2 \\times 2$.  \n",
    "El `stride`se ajusta por defecto al tamaño del kernel, por lo que esta capa utilizará un   \n",
    "stride de 2 (tanto horizontal como vertical). De forma predeterminada, utiliza un relleno     \n",
    "`VALID` (es decir, ningún relleno).\n",
    "\n",
    "- Desplegar las imágenes de entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
